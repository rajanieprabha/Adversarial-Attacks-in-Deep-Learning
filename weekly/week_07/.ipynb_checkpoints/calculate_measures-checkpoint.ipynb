{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides some implementations for different measures.\n",
    "\n",
    "Assumptions on inputs:\n",
    "The output of several MC dropout passes is: [#images x #MC x #classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by: https://github.com/lsgos/uncertainty-adversarial-paper/blob/master/src/utilities.py#L302-L332\n",
    "def entropy(X):\n",
    "    return np.sum(- X * np.log(np.clip(X, 1e-6, 1)), axis=-1)\n",
    "\n",
    "\n",
    "def expected_entropy(X):\n",
    "    \"\"\"\n",
    "    Take a tensor of MC predictions [#images x #MC x #classes] and return the\n",
    "    mean entropy of the predictive distribution across the MC samples.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.mean(entropy(X), axis=-1)\n",
    "\n",
    "\n",
    "def predictive_entropy(X):\n",
    "    \"\"\"\n",
    "    Take a tensor of MC predictions [#images x #MC x #classes] and return the\n",
    "    entropy of the mean predictive distribution across the MC samples.\n",
    "    \"\"\"\n",
    "    return entropy(np.mean(X, axis=1))\n",
    "\n",
    "def mutual_information(X):\n",
    "    \"\"\"\n",
    "    Take a tensor of MC predictions [#images x #MC x #classes] and return the\n",
    "    mutual information for each image\n",
    "    \"\"\"\n",
    "    return predictive_entropy(X) - expected_entropy(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by: https://github.com/carlini/nn_breaking_detection/blob/master/dropout_detect.py#L292-L303\n",
    "def softmax_variance(X):\n",
    "    \"\"\"\n",
    "    Take a tensor of MC predictions [#images x #MC x #classes] and return the\n",
    "    softmax variance for each image\n",
    "    \"\"\"\n",
    "    term1 = np.mean(np.sum(X**2,axis=2),axis=1)\n",
    "\n",
    "    term2 = np.sum(np.mean(X,axis=1)**2,axis=1)\n",
    "    \n",
    "    return term1-term2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#images x #MC x #classes]: (2, 3, 10)\n"
     ]
    }
   ],
   "source": [
    "nb_images = 2\n",
    "nb_classes = 10\n",
    "nb_mc = 3\n",
    "completly_random = np.random.dirichlet(np.ones(nb_classes),size=(nb_images,nb_mc)) # https://en.wikipedia.org/wiki/Dirichlet_distribution\n",
    "print(\"[#images x #MC x #classes]:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate same output for all mc passes ...\n",
    "y = np.random.dirichlet(np.ones(nb_classes),size=nb_images) # https://en.wikipedia.org/wiki/Dirichlet_distribution\n",
    "# repeat to get desired shape\n",
    "same_for_all_mc = np.repeat(y[:, np.newaxis, :], nb_mc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_some_measures(data):\n",
    "    print(\"predictive_entropy\\t\",predictive_entropy(data))\n",
    "    print(\"expected_entropy\\t\",expected_entropy(data))\n",
    "    print(\"mutual_information\\t\",mutual_information(data))\n",
    "    print(\"softmax_variance\\t\",softmax_variance(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictive_entropy\t [1.79072742 2.03247785]\n",
      "expected_entropy\t [1.79072742 2.03247785]\n",
      "mutual_information\t [ 0.0000000e+00 -4.4408921e-16]\n",
      "softmax_variance\t [0.00000000e+00 5.55111512e-17]\n"
     ]
    }
   ],
   "source": [
    "print_some_measures(same_for_all_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictive_entropy\t [2.24371702 2.06513026]\n",
      "expected_entropy\t [2.00282092 1.86947082]\n",
      "mutual_information\t [0.2408961  0.19565944]\n",
      "softmax_variance\t [0.04743172 0.04123246]\n"
     ]
    }
   ],
   "source": [
    "print_some_measures(completly_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0296530140645737\n",
      "1.0296530140645737\n"
     ]
    }
   ],
   "source": [
    "# check whether the custom entropy function produces the same as the one from scipy.stats\n",
    "x = np.array([0.2,0,0.3,0.5])\n",
    "print(scipy_entropy(x))\n",
    "print(entropy(np.array(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]]\n",
      "\n",
      " [[0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]]\n",
      "\n",
      " [[0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]]\n",
      "\n",
      " [[0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]\n",
      "  [0.2 0.  0.3 0.5]]]\n",
      "(4, 4, 4)\n",
      "[[1.38629436       -inf 1.38629436 1.38629436]\n",
      " [1.38629436       -inf 1.38629436 1.38629436]\n",
      " [1.38629436       -inf 1.38629436 1.38629436]\n",
      " [1.38629436       -inf 1.38629436 1.38629436]]\n",
      "[[1.02965301 1.02965301 1.02965301 1.02965301]\n",
      " [1.02965301 1.02965301 1.02965301 1.02965301]\n",
      " [1.02965301 1.02965301 1.02965301 1.02965301]\n",
      " [1.02965301 1.02965301 1.02965301 1.02965301]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jungj/anaconda3/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:2508: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pk = 1.0*pk / np.sum(pk, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# and for a 3D tensor...\n",
    "N = x.shape[0]\n",
    "xs = np.broadcast_to(x, (N,N,N))\n",
    "\n",
    "print(xs)\n",
    "print(xs.shape)\n",
    "\n",
    "              \n",
    "print(scipy_entropy(xs))\n",
    "print(entropy(np.array(xs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
